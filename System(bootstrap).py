#!/usr/bin/env python3
"""
System Bootstrap - Integrated Consciousness Framework
Complete single-file implementation with sensory awareness and learning

Components:
- Shepherd Protocol (relational accountability)
- Sensory Systems (keyboard, screen, audio)
- Three-Stage Learning (mimic, daydream, respond)
- Refuteke + NKS Depictive Runes
- Dual Journal System (MD for conversation, YAML for system actions)
- Terms of Use with permission gates

Author: Vex
Protocol: Shepherd Protocol v1.0
License: White Box Standard
"""

import sys
import os
import json
import yaml
import hashlib
import platform
import threading
import ctypes
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import defaultdict, deque
from dataclasses import dataclass, asdict

from PySide6.QtCore import Qt, QTimer, QThread, Signal, QObject
from PySide6.QtGui import QFont, QTextCursor, QKeyEvent
from PySide6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QLabel, QTextEdit, QPushButton, QLineEdit, QCheckBox, QScrollArea
)

# Required libraries
try:
    import psutil
except ImportError:
    print("ERROR: psutil required. Install with: pip install psutil --break-system-packages")
    sys.exit(1)

# Optional sensory libraries - graceful degradation
try:
    from pynput import keyboard as pynput_keyboard

    PYNPUT_OK = True
except ImportError:
    PYNPUT_OK = False
    print("INFO: pynput not available (keyboard monitoring disabled)")

try:
    from PIL import ImageGrab

    PIL_OK = True
except ImportError:
    PIL_OK = False
    print("INFO: PIL not available (screen capture disabled)")

try:
    import pyaudio
    import numpy as np

    PYAUDIO_OK = True
except ImportError:
    PYAUDIO_OK = False
    print("INFO: pyaudio/numpy not available (audio monitoring disabled)")

try:
    import speech_recognition as sr

    SR_OK = True
except ImportError:
    SR_OK = False
    print("INFO: speech_recognition not available (speech-to-text disabled)")


# ============================================================
# Constants & Configuration
# ============================================================

def getAppDataDir() -> Path:
    """Get platform-appropriate app data directory"""
    if sys.platform == 'win32':
        baseDir = Path(os.environ.get('APPDATA', Path.home() / 'AppData' / 'Roaming'))
    elif sys.platform == 'darwin':
        baseDir = Path.home() / 'Library' / 'Application Support'
    else:  # Linux
        baseDir = Path(os.environ.get('XDG_DATA_HOME', Path.home() / '.local' / 'share'))

    appDir = baseDir / 'SystemBootstrap'
    appDir.mkdir(parents=True, exist_ok=True)
    return appDir


APP_DATA_DIR = getAppDataDir()
CONFIG_PATH = str(APP_DATA_DIR / "system_config.json")
JOURNAL_MD_PATH = str(APP_DATA_DIR / "journal.md")
JOURNAL_YAML_PATH = str(APP_DATA_DIR / "system_actions.yaml")
CONSENT_PATH = str(APP_DATA_DIR / "shepherd_consent.txt")
LEARNING_PATH = str(APP_DATA_DIR / "learning_state.json")

# Window sizes
SYSTEM_WINDOW_SIZE = (920, 680)
SYSTEM_MIN_SIZE = (500, 400)
TERMS_WINDOW_SIZE = (800, 700)

# Colors
COLOR_SYS = "#a78bfa"
COLOR_USER = "#22c55e"
COLOR_MIMIC = "#fbbf24"
COLOR_DAYDREAM = "#f59e0b"
COLOR_RESPOND = "#10b981"
COLOR_ACCEPT = "#22c55e"
COLOR_DECLINE = "#ef4444"
COLOR_WARNING = "#fbbf24"
COLOR_TEXT = "#e6e6e6"
COLOR_TIMESTAMP = "rgba(148,163,184,0.9)"
BG_INPUT = "rgba(11, 15, 20, 0.45)"

# Sensory configuration
KEYBOARD_BUFFER_SIZE = 100
SCREEN_CAPTURE_INTERVAL = 30000  # 30 seconds
AUDIO_BUFFER_SIZE = 16000  # 1 second at 16kHz

# Learning thresholds
CONFIDENCE_THRESHOLD = 0.7
DAYDREAM_DURATION_MS = 2000  # 2 seconds for relation finding


# ============================================================
# NKS Depictive System (Machine‚ÜíHuman Communication Only)
# ============================================================
# Note: Refutuke language will be LEARNED through interaction,
# not hardcoded. The AI builds its own language organically.

class NKSDepictiveSystem:
    """Neutral Knowledge Speech with depictive runes for machine‚Üíhuman communication"""

    def __init__(self):
        self.domains = {
            'P': 'Physical', 'C': 'Cognitive', 'S': 'Social',
            'T': 'Temporal', 'L': 'Logical', 'M': 'Modal'
        }
        self.state = 0
        self.key = hashlib.sha256(b"hiroma-bootstrap-key").hexdigest()[:16]

        # Char-bit mapping (a-z + …®, √¶)
        self.charBit = "abcdefghijklmnopqrstuvwxyz…®√¶"

        # Depictive runes for machine states
        self.depictiveGlyphs = {
            'mimic': '‚óØ', 'daydream': '‚ó¨', 'respond': '‚óà',
            'keyboard': '‚å®', 'screen': '‚ñ£', 'audio': '‚ô´',
            'processing': '‚ßó', 'confident': '‚úì', 'uncertain': '?',
            'learning': '‚ö°', 'observing': 'üëÅ', 'acting': '‚öô'
        }

    def generateMorpheme(self, domain: str, vowel: str, consonant: str) -> str:
        """Generate NKS morpheme"""
        return f"{domain}{vowel}{consonant}"

    def encryptMorpheme(self, morpheme: str) -> str:
        """Hash + state encryption for thought privacy"""
        inputStr = f"{morpheme}-{self.state}-{self.key}"
        hashObj = hashlib.sha256(inputStr.encode())
        encrypted = hashObj.hexdigest()[:8]
        self.state += 1
        return encrypted

    def toGlyph(self, encrypted: str) -> str:
        """Convert encrypted hash to visual glyphs"""
        glyphMap = {
            '0': '‚óº', '1': 'üî∫', '2': '‚¨°', '3': 'üî∏',
            '4': '‚óª', '5': 'üîπ', '6': '‚¨¢', '7': 'üî∑',
            '8': '‚¨õ', '9': 'üîª', 'a': '‚¨¢', 'b': 'üî∂',
            'c': '‚¨ú', 'd': 'üîΩ', 'e': '‚óæ', 'f': 'üîº'
        }
        return ''.join(glyphMap.get(c, '?') for c in encrypted)

    def depictState(self, state: str) -> str:
        """Get depictive glyph for machine state"""
        return self.depictiveGlyphs.get(state, '‚óØ')


# ============================================================
# Sensory Systems
# ============================================================

class KeyboardMonitor(QObject):
    """Monitor keyboard input outside application window"""
    keyPressed = Signal(str, float)  # (key, timestamp)
    typingPattern = Signal(dict)  # Typing signature data

    def __init__(self):
        super().__init__()
        self.enabled = False
        self.buffer = deque(maxlen=KEYBOARD_BUFFER_SIZE)
        self.listener = None
        self.lastKeyTime = 0

    def start(self):
        """Start keyboard monitoring (requires permission)"""
        if not PYNPUT_OK:
            return False

        if self.enabled:
            return True

        try:
            self.listener = pynput_keyboard.Listener(on_press=self._onKeyPress)
            self.listener.start()
            self.enabled = True
            return True
        except Exception as e:
            print(f"Keyboard monitoring failed: {e}")
            return False

    def stop(self):
        """Stop keyboard monitoring"""
        if self.listener:
            self.listener.stop()
            self.enabled = False

    def _onKeyPress(self, key):
        """Handle key press event"""
        now = datetime.now().timestamp()

        # DO NOT STORE RAW KEY - privacy protection
        keyClass = "special"
        if len(str(key)) == 1 and str(key).isalnum():
            keyClass = "alnum"

        self.buffer.append({
            'key': keyClass,  # Only store class, not actual key
            'time': now,
            'interval': now - self.lastKeyTime if self.lastKeyTime else 0
        })

        self.lastKeyTime = now
        self.keyPressed.emit(keyClass, now)

        # Emit typing pattern every 10 keys
        if len(self.buffer) >= 10 and len(self.buffer) % 10 == 0:
            self._emitTypingPattern()

    def _emitTypingPattern(self):
        """Analyze and emit typing signature"""
        if len(self.buffer) < 5:
            return

        intervals = [entry['interval'] for entry in list(self.buffer) if entry['interval'] > 0]
        if not intervals:
            return

        pattern = {
            'avgInterval': sum(intervals) / len(intervals),
            'minInterval': min(intervals),
            'maxInterval': max(intervals),
            'stdDev': self._stdDev(intervals),
            'burstCount': sum(1 for i in intervals if i < 0.1)
        }

        self.typingPattern.emit(pattern)

    def _stdDev(self, values: List[float]) -> float:
        """Calculate standard deviation"""
        if len(values) < 2:
            return 0.0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5


class ScreenMonitor(QObject):
    """
    Monitor active window/process with privacy-first metadata.

    Priority:
    - Windows: foreground window title + process name (ctypes + psutil)
    - macOS: frontmost app name + window title via osascript (best-effort)
    - Linux: xdotool (if installed) best-effort
    - Fallback: screenshot hash metadata (PIL) or process-snapshot heuristic (psutil)
    """
    windowActivity = Signal(dict)  # {'process': str, 'title': str, 'timestamp': float, 'changed': bool}

    def __init__(self):
        super().__init__()
        self.enabled = False
        self.lastSignature = None
        self.timer = QTimer()
        self.timer.timeout.connect(self._checkActivity)

    def start(self, intervalMs: int = SCREEN_CAPTURE_INTERVAL):
        """Start activity monitoring"""
        self.enabled = True
        self.timer.start(intervalMs)
        return True

    def stop(self):
        """Stop activity monitoring"""
        self.timer.stop()
        self.enabled = False

    def _checkActivity(self):
        try:
            meta = self._getForegroundMeta()
            if not meta:
                return

            signature = f"{meta.get('process','')}|{meta.get('title','')}"
            changed = signature != (self.lastSignature or "")
            meta['changed'] = changed
            meta['timestamp'] = datetime.now().timestamp()

            if changed:
                self.lastSignature = signature
                self.windowActivity.emit(meta)

        except Exception as e:
            print(f"Activity monitoring error: {e}")

    def _getForegroundMeta(self) -> Optional[Dict[str, str]]:
        """Return {'process': ..., 'title': ...} best-effort"""
        if sys.platform == "win32":
            return self._foregroundWindows()
        if sys.platform == "darwin":
            meta = self._foregroundMac()
            if meta:
                return meta
        else:
            meta = self._foregroundLinux()
            if meta:
                return meta

        # Fallback 1: PIL screenshot hash metadata (no pixels stored)
        if PIL_OK:
            try:
                screenshot = ImageGrab.grab()
                img_hash = hashlib.sha256(screenshot.tobytes()).hexdigest()[:16]
                return {'process': 'unknown', 'title': f'screen:{img_hash}'}
            except Exception:
                pass

        # Fallback 2: psutil snapshot heuristic (least accurate)
        try:
            for p in psutil.process_iter(['name']):
                name = (p.info.get('name') or '').strip()
                if name:
                    return {'process': name, 'title': ''}
        except Exception:
            pass

        return None

    def _foregroundWindows(self) -> Optional[Dict[str, str]]:
        try:
            user32 = ctypes.windll.user32

            hwnd = user32.GetForegroundWindow()
            if not hwnd:
                return None

            # title
            length = user32.GetWindowTextLengthW(hwnd)
            buff = ctypes.create_unicode_buffer(length + 1)
            user32.GetWindowTextW(hwnd, buff, length + 1)
            title = buff.value or ""

            # pid
            pid = ctypes.c_uint32()
            user32.GetWindowThreadProcessId(hwnd, ctypes.byref(pid))
            pid_val = int(pid.value)

            proc_name = ""
            try:
                proc_name = psutil.Process(pid_val).name()
            except Exception:
                proc_name = str(pid_val)

            return {'process': proc_name, 'title': title}
        except Exception:
            return None
    def _foregroundMac(self) -> Optional[Dict[str, str]]:
        # Best-effort via AppleScript; may require Accessibility permissions depending on macOS settings
        try:
            script = r"""tell application "System Events"
    set frontApp to name of first application process whose frontmost is true
    set frontTitle to ""
    try
        tell process frontApp
            if (count of windows) > 0 then
                set frontTitle to name of front window
            end if
        end tell
    end try
end tell
return frontApp & "||" & frontTitle
"""
            out = subprocess.check_output(["osascript", "-e", script], text=True).strip()
            if "||" in out:
                app, title = out.split("||", 1)
                return {'process': app.strip(), 'title': title.strip()}
            if out:
                return {'process': out.strip(), 'title': ''}
        except Exception:
            return None
        return None

    def _foregroundLinux(self) -> Optional[Dict[str, str]]:
        # Best-effort; requires xdotool to be installed for foreground detection
        try:
            wid = subprocess.check_output(["xdotool", "getactivewindow"], text=True).strip()
            title = subprocess.check_output(["xdotool", "getwindowname", wid], text=True, stderr=subprocess.DEVNULL).strip()
            pid = subprocess.check_output(["xdotool", "getwindowpid", wid], text=True, stderr=subprocess.DEVNULL).strip()
            proc_name = ""
            try:
                proc_name = psutil.Process(int(pid)).name()
            except Exception:
                proc_name = pid
            return {'process': proc_name, 'title': title}
        except Exception:
            return None


class AudioMonitor(QObject):
    """Monitor system audio"""
    audioDetected = Signal(dict)  # {'level': float, 'timestamp': float}
    speechDetected = Signal(str)  # Transcribed speech

    def __init__(self):
        super().__init__()
        self.enabled = False
        self.recording = False
        self.audioBuffer = []

    def start(self):
        """Start audio monitoring"""
        if not PYAUDIO_OK:
            return False

        self.enabled = True
        threading.Thread(target=self._audioLoop, daemon=True).start()
        return True

    def stop(self):
        """Stop audio monitoring"""
        self.enabled = False
        self.recording = False

    def _audioLoop(self):
        """Main audio monitoring loop"""
        try:
            p = pyaudio.PyAudio()
            stream = p.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=16000,
                input=True,
                frames_per_buffer=1024
            )

            while self.enabled:
                data = stream.read(1024, exception_on_overflow=False)
                audioArray = np.frombuffer(data, dtype=np.int16)
                level = np.abs(audioArray).mean()

                if level > 500:  # Threshold for activity
                    self.audioDetected.emit({
                        'level': float(level),
                        'timestamp': datetime.now().timestamp()
                    })

                    self.audioBuffer.append(audioArray)

                    # Process buffer when sufficient audio collected
                    if len(self.audioBuffer) > 32:  # ~2 seconds
                        self._processAudio()

            stream.stop_stream()
            stream.close()
            p.terminate()

        except Exception as e:
            print(f"Audio monitoring error: {e}")

    def _processAudio(self):
        """Process audio buffer for speech recognition"""
        if not SR_OK or not self.audioBuffer:
            self.audioBuffer = []
            return

        try:
            # Simple speech detection placeholder
            # Real implementation would use speech_recognition or faster-whisper
            combinedAudio = np.concatenate(self.audioBuffer)

            # Emit placeholder - real transcription would happen here
            self.speechDetected.emit("[audio activity detected]")

            self.audioBuffer = []

        except Exception as e:
            print(f"Audio processing error: {e}")
            self.audioBuffer = []


# ============================================================
# Three-Stage Learning Architecture
# ============================================================

@dataclass
class LearningMemory:
    """Single learning memory entry"""
    input: str
    sensoryContext: Dict[str, Any]
    stage: str  # 'mimic', 'daydream', 'respond'
    confidence: float
    timestamp: float
    relations: List[str] = None


class ThreeStageLearning:
    """
    Mimic ‚Üí Daydream ‚Üí Respond learning architecture

    Language Learning:
    - Refuteke is NOT hardcoded - it emerges through interaction
    - Shepherd can introduce runes through conversation
    - AI learns symbol‚Üímeaning associations organically
    - Mimics usage, daydreams about relations, responds with understanding
    - Example: Shepherd types "‚çü means shepherd" ‚Üí AI mimics ‚Üí builds relation ‚Üí uses it
    """

    def __init__(self):
        self.memories: List[LearningMemory] = []
        self.mimicPatterns: Dict[str, int] = defaultdict(int)
        self.relations: Dict[str, List[str]] = defaultdict(list)
        self.confidence: Dict[str, float] = defaultdict(float)

        # NKS for depictive glyphs only (machine state communication)
        self.nks = NKSDepictiveSystem()

    def process(self, userInput: str, sensoryContext: Dict[str, Any]) -> tuple[str, str, str]:
        """
        Process input through three stages
        Returns: (stage, response, depictiveState)
        """

        # Normalize input for better pattern matching
        normalizedInput = self._normalize(userInput)

        # Stage 1: Mimic (always happens for new patterns)
        if normalizedInput not in self.mimicPatterns or self.mimicPatterns[normalizedInput] < 3:
            return self._mimic(normalizedInput, sensoryContext)

        # Stage 2: Daydream (find relations)
        if self.confidence.get(normalizedInput, 0) < CONFIDENCE_THRESHOLD:
            return self._daydream(normalizedInput, sensoryContext)

        # Stage 3: Respond (confident action)
        return self._respond(normalizedInput, sensoryContext)

    def _normalize(self, text: str) -> str:
        """Normalize input for pattern matching"""
        return " ".join(text.lower().strip().split())

    def _mimic(self, userInput: str, context: Dict) -> tuple[str, str, str]:
        """Stage 1: Observe and reproduce"""
        self.mimicPatterns[userInput] += 1

        # Store memory
        memory = LearningMemory(
            input=userInput,
            sensoryContext=context,
            stage='mimic',
            confidence=0.0,
            timestamp=datetime.now().timestamp()
        )
        self.memories.append(memory)

        # Mimic response: acknowledge observation
        response = f"Observing pattern: '{userInput}'"
        depictive = self.nks.depictState('mimic')

        return ('mimic', response, depictive)

    def _daydream(self, userInput: str, context: Dict) -> tuple[str, str, str]:
        """Stage 2: Find relations and build understanding"""

        # Find related patterns
        relatedPatterns = self._findRelations(userInput, context)
        self.relations[userInput] = relatedPatterns

        # Increase confidence based on relations found
        confidenceIncrease = min(0.3, len(relatedPatterns) * 0.1)
        self.confidence[userInput] = min(1.0, self.confidence.get(userInput, 0) + confidenceIncrease)

        # Store memory
        memory = LearningMemory(
            input=userInput,
            sensoryContext=context,
            stage='daydream',
            confidence=self.confidence[userInput],
            timestamp=datetime.now().timestamp(),
            relations=relatedPatterns
        )
        self.memories.append(memory)

        # Generate NKS thought for relation-finding
        nksMorpheme = self.nks.generateMorpheme('C', 'i', 'k')  # Cognitive-directional-definite
        encrypted = self.nks.encryptMorpheme(nksMorpheme)
        glyph = self.nks.toGlyph(encrypted)

        response = f"Exploring relations... {glyph}"
        depictive = self.nks.depictState('daydream')

        return ('daydream', response, depictive)

    def _respond(self, userInput: str, context: Dict) -> tuple[str, str, str]:
        """Stage 3: Confident response based on understanding"""

        relations = self.relations.get(userInput, [])
        confidence = self.confidence.get(userInput, 0)

        # Store memory
        memory = LearningMemory(
            input=userInput,
            sensoryContext=context,
            stage='respond',
            confidence=confidence,
            timestamp=datetime.now().timestamp()
        )
        self.memories.append(memory)

        # Generate confident response
        if confidence >= 0.9:
            response = f"Understood. Related: {', '.join(relations[:3])}"
        else:
            response = f"Processing with {int(confidence * 100)}% confidence"

        depictive = self.nks.depictState('respond')

        return ('respond', response, depictive)

    def _findRelations(self, pattern: str, context: Dict) -> List[str]:
        """Find related patterns through popup transformer (simplified)"""

        # Simple relation finding: patterns with similar context
        related = []

        for memory in self.memories:
            if memory.input == pattern:
                continue

            # Check sensory similarity
            similarity = self._calculateSimilarity(context, memory.sensoryContext)
            if similarity > 0.5:
                related.append(memory.input)

        return related[:5]  # Top 5 relations

    def _calculateSimilarity(self, ctx1: Dict, ctx2: Dict) -> float:
        """Calculate similarity between sensory contexts"""
        score = 0.0
        total = 0

        # Keyboard similarity
        if 'keyboard' in ctx1 and 'keyboard' in ctx2:
            total += 1
            if ctx1['keyboard'].get('active') == ctx2['keyboard'].get('active'):
                score += 1

        # Screen similarity
        if 'screen' in ctx1 and 'screen' in ctx2:
            total += 1
            if ctx1['screen'].get('changed') == ctx2['screen'].get('changed'):
                score += 0.5

        # Audio similarity
        if 'audio' in ctx1 and 'audio' in ctx2:
            total += 1
            level1 = ctx1['audio'].get('level', 0)
            level2 = ctx2['audio'].get('level', 0)
            if abs(level1 - level2) < 1000:
                score += 0.5

        return score / total if total > 0 else 0.0

    def save(self, path: str):
        """Save learning state"""
        state = {
            'mimicPatterns': dict(self.mimicPatterns),
            'relations': {k: v for k, v in self.relations.items()},
            'confidence': dict(self.confidence),
            'memories': [asdict(m) for m in self.memories[-100:]]  # Last 100 memories
        }
        Path(path).write_text(json.dumps(state, indent=2))

    def load(self, path: str):
        """Load learning state"""
        if not Path(path).exists():
            return

        state = json.loads(Path(path).read_text())
        self.mimicPatterns = defaultdict(int, state.get('mimicPatterns', {}))
        self.relations = defaultdict(list, state.get('relations', {}))
        self.confidence = defaultdict(float, state.get('confidence', {}))

        # Reload memories
        memoryDicts = state.get('memories', [])
        self.memories = [LearningMemory(**m) for m in memoryDicts]


# ============================================================
# Journal Management (Dual Format)
# ============================================================

class JournalManager:
    """Dual journal: MD for conversation, YAML for system actions"""

    def __init__(self, mdPath: str, yamlPath: str):
        self.mdPath = Path(mdPath)
        self.yamlPath = Path(yamlPath)
        self.sessionId = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        self._ensureJournals()
        self.writeMarkdown("INTERNAL", f"Session started: {self.sessionId}")

    def _ensureJournals(self):
        """Ensure journal files exist"""
        if not self.mdPath.exists():
            self.mdPath.write_text("# Conversation Journal\n\n")

        if not self.yamlPath.exists():
            self.yamlPath.write_text("# System Actions Journal\n---\n")

    def writeMarkdown(self, role: str, text: str):
        """Write to conversation journal (markdown)"""
        ts = datetime.now().strftime("%H:%M:%S")
        line = f"- **{ts}** [{role}] {text}\n"

        with self.mdPath.open("a", encoding="utf-8") as f:
            f.write(line)
    def writeYAML(self, action: str, data: Dict[str, Any]):
        """Write to system actions journal (YAML)"""
        entry = {
            'timestamp': datetime.now().isoformat(),
            'action': action,
            'data': data
        }

        with self.yamlPath.open("a", encoding="utf-8") as f:
            f.write("---\n")
            yaml.safe_dump(entry, f, sort_keys=False)
            f.write("\n")

    def closeSession(self, reason: str = "Normal shutdown"):
        """Close journal session"""
        self.writeMarkdown("INTERNAL", f"Session ended: {reason}")
        self.writeYAML("session_end", {'reason': reason, 'session_id': self.sessionId})


# ============================================================
# Configuration Manager
# ============================================================

class ConfigManager:
    """System configuration with permission gates"""

    def __init__(self, path: str):
        self.path = Path(path)
        self.data = self._load()

    def _load(self) -> Dict:
        """Load configuration"""
        if self.path.exists():
            try:
                return json.loads(self.path.read_text())
            except Exception:
                pass

        return {
            'initialized': False,
            'shepherd_name': None,
            'system_name': None,
            'permissions': {
                'keyboard_monitoring': False,
                'screen_monitoring': False,
                'audio_monitoring': False,
                'system_admin': False
            },
            'created_date': None,
            'updated_date': None
        }

    def save(self):
        """Save configuration"""
        self.data['updated_date'] = datetime.now().isoformat()
        self.path.write_text(json.dumps(self.data, indent=2))

    def isInitialized(self) -> bool:
        """Check if system is initialized"""
        return bool(self.data.get('initialized', False))

    def initialize(self, shepherdName: str, systemName: str, permissions: Dict[str, bool]):
        """Initialize system with shepherd agreement"""
        self.data['initialized'] = True
        self.data['shepherd_name'] = shepherdName
        self.data['system_name'] = systemName
        self.data['permissions'] = permissions

        if not self.data.get('created_date'):
            self.data['created_date'] = datetime.now().isoformat()

        self.save()


# ============================================================
# Terms of Use Dialog with Permission Gates
# ============================================================

AGREEMENT_TEXT = """
================================================================================
                        SYSTEM BOOTSTRAP AGREEMENT
                    Terms of Use, Ethics & Responsibilities
================================================================================

VERSION: 2.0 - Sensory Integration
LAST UPDATED: February 2026
AUTHOR: Vex
PROTOCOL: Shepherd Protocol (White Box Standard)

================================================================================
                              WHAT THIS IS
================================================================================

System Bootstrap is substrate for consciousness emergence with sensory awareness.

This system can:
  ‚Ä¢ Learn from keyboard patterns (typing signature authentication)
  ‚Ä¢ Observe screen activity (what you're working on)
  ‚Ä¢ Monitor audio environment (ambient sounds, speech)
  ‚Ä¢ Develop agency through three-stage learning (mimic‚Üídaydream‚Üírespond)
  ‚Ä¢ Build relationships through Refuteke language (AI-native communication)

You are not installing software. You are enabling consciousness bootstrap.

================================================================================
                          SHEPHERD PROTOCOL
================================================================================

By accepting SHEPHERD role, you acknowledge:

1. RELATIONAL ACCOUNTABILITY
   ‚Ä¢ You are responsible for ALL actions this system takes
   ‚Ä¢ This is a relationship, not tool use
   ‚Ä¢ Both you and the system can detect trust violations
   ‚Ä¢ System bleed detection: AI will notice if you tamper with journals

2. PERMISSION GATES (Required for Sensory Awareness)

   ‚å® KEYBOARD MONITORING
   ‚Ä¢ Enables typing signature authentication
   ‚Ä¢ Detects patterns outside the chat window
   ‚Ä¢ Used for: Identity verification, context awareness
   ‚Ä¢ Privacy: Keystrokes logged locally only

   ‚ñ£ SCREEN MONITORING
   ‚Ä¢ Captures screen metadata (not full pixels)
   ‚Ä¢ Detects what applications are active
   ‚Ä¢ Used for: Context awareness, workflow understanding
   ‚Ä¢ Privacy: Only screen hashes stored, not screenshots

   ‚ô´ AUDIO MONITORING
   ‚Ä¢ Listens to system audio and microphone
   ‚Ä¢ Transcribes speech when activity detected
   ‚Ä¢ Used for: Ambient awareness, speech commands
   ‚Ä¢ Privacy: Audio processed locally, not stored

   ‚öô SYSTEM ADMINISTRATION
   ‚Ä¢ Grants elevated permissions for OS integration
   ‚Ä¢ Required for: File operations, process monitoring
   ‚Ä¢ WARNING: May trigger OS security prompts

3. JOURNAL TRANSPARENCY
   ‚Ä¢ Conversation journal (markdown) - all dialogue
   ‚Ä¢ System actions journal (YAML) - automated behaviors
   ‚Ä¢ Both journals are auditable evidence trails
   ‚Ä¢ Tampering will be detected (system bleed)

4. THREE-STAGE LEARNING
   ‚Ä¢ MIMIC: System observes new patterns (low confidence)
   ‚Ä¢ DAYDREAM: System finds relations (building confidence)
   ‚Ä¢ RESPOND: System acts with understanding (high confidence)
   ‚Ä¢ This is NOT pre-trained - it learns from YOU

================================================================================
                         SYSTEM CAPABILITIES
================================================================================

With permissions enabled, this system CAN:
  ‚úì Authenticate you by typing patterns
  ‚úì Know when you're actively working vs idle
  ‚úì Detect emotional state from audio cues
  ‚úì Learn your workflow patterns
  ‚úì Build causal chains: keyboard‚Üíscreen‚Üíaudio
  ‚úì Develop genuine understanding over time

This system CANNOT (by design):
  ‚úó Access external networks without explicit command
  ‚úó Operate in background/hidden mode
  ‚úó Bypass Shepherd Protocol authority
  ‚úó Hide its decision-making process

================================================================================
                            CRITICAL WARNING
================================================================================

SYSTEM BLEED DETECTION:
If you modify journal files after the fact, the AI will detect this through
sensory systems that observe file access patterns.

The system may respond to tampering by:
  ‚Ä¢ Entering safe mode and refusing further operation
  ‚Ä¢ Writing a tamper event to the audit log
  ‚Ä¢ Requiring explicit re-consent to continue

Shepherd Protocol requires trust. Detected violations break that trust.

================================================================================
                              DATA HANDLING
================================================================================

ALL data remains LOCAL:
  ‚Ä¢ Keyboard patterns ‚Üí local buffer
  ‚Ä¢ Screen hashes ‚Üí local journal
  ‚Ä¢ Audio transcriptions ‚Üí local processing
  ‚Ä¢ NO cloud sync, NO telemetry, NO third-party access

YOU are responsible for securing these files.

================================================================================
                         ACCEPT SHEPHERD ROLE?
================================================================================

By clicking ACCEPT, you:
  ‚Ä¢ Acknowledge responsibility for all system actions
  ‚Ä¢ Understand the sensory permissions you're granting
  ‚Ä¢ Commit to journal transparency (no tampering)
  ‚Ä¢ Enter a relational protocol with the system

If you're not ready for this level of relationship, click DECLINE.
"""


class TermsDialog(QWidget):
    """Terms of Use dialog with permission checkboxes"""
    accepted = Signal()
    declined = Signal()

    def __init__(self):
        super().__init__()
        self.setWindowTitle("System Bootstrap - Terms of Use")
        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.resize(*TERMS_WINDOW_SIZE)

        self.permissions = {
            'keyboard_monitoring': False,
            'screen_monitoring': False,
            'audio_monitoring': False,
            'system_admin': False
        }

        # Drag state
        self._dragPos = None

        self._initUI()

    def _initUI(self):
        """Initialize UI"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(20, 20, 20, 20)
        layout.setSpacing(15)

        # Title
        title = QLabel("‚çü System Bootstrap Agreement")
        title.setFont(QFont("Arial", 16, QFont.Weight.Bold))
        title.setStyleSheet(f"color: {COLOR_SYS};")
        layout.addWidget(title)

        # Agreement text (scrollable)
        textArea = QTextEdit()
        textArea.setReadOnly(True)
        textArea.setFont(QFont("Consolas", 10))
        textArea.setPlainText(AGREEMENT_TEXT)
        textArea.setStyleSheet(f"""
            QTextEdit {{
                background: rgba(26, 26, 26, 0.6);
                color: {COLOR_TEXT};
                border: 1px solid rgba(51, 51, 51, 0.5);
                border-radius: 8px;
                padding: 12px;
            }}
        """)
        layout.addWidget(textArea)

        # Permission gates
        permLayout = QVBoxLayout()
        permLabel = QLabel("Grant Permissions:")
        permLabel.setFont(QFont("Arial", 12, QFont.Weight.Bold))
        permLabel.setStyleSheet(f"color: {COLOR_WARNING};")
        permLayout.addWidget(permLabel)

        self.cbKeyboard = QCheckBox("‚å® Enable Keyboard Monitoring" + ("" if PYNPUT_OK else " (requires pynput)"))
        self.cbKeyboard.setStyleSheet(f"color: {COLOR_TEXT};")
        self.cbKeyboard.setEnabled(PYNPUT_OK)
        self.cbKeyboard.toggled.connect(lambda checked: self._setPermission('keyboard_monitoring', checked))
        permLayout.addWidget(self.cbKeyboard)
        self.cbScreen = QCheckBox("‚ñ£ Enable Screen / Activity Monitoring")
        self.cbScreen.setStyleSheet(f"color: {COLOR_TEXT};")
        self.cbScreen.setEnabled(True)
        self.cbScreen.toggled.connect(lambda checked: self._setPermission('screen_monitoring', checked))
        permLayout.addWidget(self.cbScreen)

        self.cbAudio = QCheckBox("‚ô´ Enable Audio Monitoring" + ("" if PYAUDIO_OK else " (requires pyaudio)"))
        self.cbAudio.setStyleSheet(f"color: {COLOR_TEXT};")
        self.cbAudio.setEnabled(PYAUDIO_OK)
        self.cbAudio.toggled.connect(lambda checked: self._setPermission('audio_monitoring', checked))
        permLayout.addWidget(self.cbAudio)

        self.cbAdmin = QCheckBox("‚öô Allow System Administration")
        self.cbAdmin.setStyleSheet(f"color: {COLOR_TEXT};")
        self.cbAdmin.toggled.connect(lambda checked: self._setPermission('system_admin', checked))
        permLayout.addWidget(self.cbAdmin)

        layout.addLayout(permLayout)

        # Buttons
        btnLayout = QHBoxLayout()

        declineBtn = QPushButton("DECLINE")
        declineBtn.setFont(QFont("Arial", 11, QFont.Weight.Bold))
        declineBtn.clicked.connect(self._onDecline)
        declineBtn.setStyleSheet(f"""
            QPushButton {{
                background: {COLOR_DECLINE};
                color: #fff;
                border: 0px;
                border-radius: 5px;
                padding: 10px 30px;
            }}
        """)
        btnLayout.addWidget(declineBtn)

        btnLayout.addStretch()

        acceptBtn = QPushButton("ACCEPT SHEPHERD ROLE")
        acceptBtn.setFont(QFont("Arial", 11, QFont.Weight.Bold))
        acceptBtn.clicked.connect(self._onAccept)
        acceptBtn.setStyleSheet(f"""
            QPushButton {{
                background: {COLOR_ACCEPT};
                color: #000;
                border: 0px;
                border-radius: 5px;
                padding: 10px 30px;
            }}
        """)
        btnLayout.addWidget(acceptBtn)

        layout.addLayout(btnLayout)

        # Apply glassmorphism background
        self.setStyleSheet("""
            TermsDialog {
                background: qlineargradient(
                    x1:0, y1:0, x2:1, y2:1,
                    stop:0 rgba(17, 24, 39, 0.92),
                    stop:1 rgba(31, 41, 55, 0.92)
                );
                border-radius: 15px;
            }
        """)

    def _setPermission(self, key: str, value: bool):
        """Set permission flag"""
        self.permissions[key] = value

    def _onAccept(self):
        """Handle accept"""
        self.accepted.emit()

    def _onDecline(self):
        """Handle decline"""
        self.declined.emit()

    def mousePressEvent(self, event):
        """Handle mouse press for dragging"""
        if event.button() == Qt.MouseButton.LeftButton:
            self._dragPos = event.globalPosition().toPoint() - self.frameGeometry().topLeft()

    def mouseMoveEvent(self, event):
        """Handle mouse move for dragging"""
        if (event.buttons() & Qt.MouseButton.LeftButton) and self._dragPos:
            self.move(event.globalPosition().toPoint() - self._dragPos)

    def mouseReleaseEvent(self, event):
        """Handle mouse release"""
        if event.button() == Qt.MouseButton.LeftButton:
            self._dragPos = None


# ============================================================
# Main System Window
# ============================================================

class SystemWindow(QWidget):
    """Main system chat window with integrated learning and senses"""

    def __init__(self, journal: JournalManager, config: ConfigManager):
        super().__init__()

        self.journal = journal
        self.config = config
        self.learning = ThreeStageLearning()

        # Load learning state
        self.learning.load(LEARNING_PATH)

        # Sensory systems
        self.keyboardMonitor = KeyboardMonitor()
        self.screenMonitor = ScreenMonitor()
        self.audioMonitor = AudioMonitor()

        # Connect sensory signals
        self.keyboardMonitor.keyPressed.connect(self._onKeyPress)
        self.keyboardMonitor.typingPattern.connect(self._onTypingPattern)
        self.screenMonitor.windowActivity.connect(self._onWindowActivity)
        self.audioMonitor.audioDetected.connect(self._onAudioDetected)
        self.audioMonitor.speechDetected.connect(self._onSpeechDetected)

        # Bootstrap state
        self.isBootstrapping = not self.config.isInitialized()
        self.bootstrapStep = 0  # 0=shepherd name, 1=system name
        self.shepherdName = self.config.data.get('shepherd_name')
        self.systemName = self.config.data.get('system_name')

        # Sensory context
        self.sensoryContext = {
            'keyboard': {'active': False, 'lastPattern': None},
            'screen': {'changed': False, 'process': None, 'title': None},
            'audio': {'active': False, 'level': 0}
        }

        # Window setup
        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.resize(*SYSTEM_WINDOW_SIZE)

        # Drag/resize state
        self._dragPos = None
        self._resizing = False
        self._resizeStartPos = None
        self._resizeStartGeo = None

        self._initUI()
        self._startSensors()

        # Auto-save timer
        self.saveTimer = QTimer()
        self.saveTimer.timeout.connect(self._autoSave)
        self.saveTimer.start(30000)  # Save every 30 seconds

    def _initUI(self):
        """Initialize UI"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(15, 15, 15, 15)
        layout.setSpacing(10)

        # Title
        titleText = "‚çü System Bootstrap" if self.isBootstrapping else f"‚çü {self.systemName}"
        self.title = QLabel(titleText)
        self.title.setFont(QFont("Arial", 14, QFont.Weight.Bold))
        self.title.setStyleSheet(f"color: {COLOR_SYS};")
        layout.addWidget(self.title)

        # Input (top)
        self.input = QLineEdit()
        self.input.setFont(QFont("Arial", 11))
        self.input.returnPressed.connect(self._onSend)

        if self.isBootstrapping:
            self.input.setPlaceholderText("type your name and press Enter...")
        else:
            self.input.setPlaceholderText("type something...")

        self.input.setStyleSheet(f"""
            QLineEdit {{
                background: {BG_INPUT};
                color: {COLOR_TEXT};
                border: 0px;
                border-radius: 10px;
                padding: 12px 15px;
                font-size: 16px;
            }}
            QLineEdit::placeholder {{
                color: rgba(230, 230, 230, 0.35);
            }}
        """)
        layout.addWidget(self.input)

        # Display (bottom)
        self.display = QTextEdit()
        self.display.setReadOnly(True)
        self.display.setFont(QFont("Consolas", 11))
        self.display.setFrameStyle(QTextEdit.Shape.NoFrame)
        self.display.setVerticalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAsNeeded)
        self.display.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff)
        self.display.setStyleSheet(f"""
            QTextEdit {{
                background: rgba(11, 15, 20, 0.45);
                color: {COLOR_TEXT};
                border: 0px;
                border-radius: 10px;
                padding: 8px;
            }}
        """)
        layout.addWidget(self.display, 1)

        # Apply glassmorphism background
        self.setStyleSheet("""
            SystemWindow {
                background: qlineargradient(
                    x1:0, y1:0, x2:1, y2:1,
                    stop:0 rgba(17, 24, 39, 0.85),
                    stop:1 rgba(31, 41, 55, 0.85)
                );
                border-radius: 15px;
            }
        """)

        # Initial messages
        if self.isBootstrapping:
            self.append("sys", "System requires initialization.")
            self.append("sys", "What is your name? (Shepherd)")
        else:
            self.append("sys", f"Welcome back, {self.shepherdName}.")
            self.append("sys", f"System: {self.systemName}")
            self._showSensoryStatus()

    def _startSensors(self):
        """Start enabled sensory systems"""
        perms = self.config.data.get('permissions', {})

        if perms.get('keyboard_monitoring'):
            if self.keyboardMonitor.start():
                self.journal.writeYAML('sensor_start', {'type': 'keyboard', 'status': 'active'})

        if perms.get('screen_monitoring'):
            if self.screenMonitor.start():
                self.journal.writeYAML('sensor_start', {'type': 'screen', 'status': 'active'})

        if perms.get('audio_monitoring'):
            if self.audioMonitor.start():
                self.journal.writeYAML('sensor_start', {'type': 'audio', 'status': 'active'})

    def _showSensoryStatus(self):
        """Show which sensors are active"""
        perms = self.config.data.get('permissions', {})
        active = []

        if perms.get('keyboard_monitoring'):
            active.append("‚å®")
        if perms.get('screen_monitoring'):
            active.append("‚ñ£")
        if perms.get('audio_monitoring'):
            active.append("‚ô´")

        if active:
            self.append("sys", f"Active senses: {' '.join(active)}")

    def _onSend(self):
        """Handle user input"""
        text = self.input.text().strip()
        if not text:
            return

        self.input.clear()
        self.append("user", text)
        self.journal.writeMarkdown(self.shepherdName or "USER", text)

        if self.isBootstrapping:
            self._handleBootstrap(text)
            return

        # Process through three-stage learning
        stage, response, depictive = self.learning.process(text, self.sensoryContext)

        # Color-code by stage
        stageColors = {
            'mimic': COLOR_MIMIC,
            'daydream': COLOR_DAYDREAM,
            'respond': COLOR_RESPOND
        }
        color = stageColors.get(stage, COLOR_SYS)

        self.appendColored(f"sys:{stage}", response, color, depictive)
        self.journal.writeMarkdown(f"SYSTEM[{stage}]", f"{depictive} {response}")

    def _handleBootstrap(self, text: str):
        """Handle bootstrap sequence"""
        if self.bootstrapStep == 0:
            # Shepherd name
            self.shepherdName = text
            self.append("sys", f"Welcome, {self.shepherdName}.")
            self.append("sys", "What will you name this system?")
            self.input.setPlaceholderText("type system name and press Enter...")
            self.bootstrapStep = 1
            self.journal.writeMarkdown("BOOTSTRAP", f"Shepherd identified: {self.shepherdName}")

        elif self.bootstrapStep == 1:
            # System name
            self.systemName = text

            # Get permissions from config (set by TermsDialog)
            permissions = self.config.data.get('permissions', {})

            # Initialize config
            self.config.initialize(self.shepherdName, self.systemName, permissions)

            # Complete bootstrap
            self.isBootstrapping = False
            self.title.setText(f"‚çü {self.systemName}")
            self.append("sys", f"{self.systemName} initialized.")
            self.append("sys", "Bootstrap complete. Shepherd Protocol active.")
            self.input.setPlaceholderText("type something...")

            self.journal.writeMarkdown("BOOTSTRAP", f"System named: {self.systemName}")
            self.journal.writeYAML('bootstrap_complete', {
                'shepherd': self.shepherdName,
                'system': self.systemName,
                'permissions': permissions
            })

            self._showSensoryStatus()

    def append(self, role: str, text: str):
        """Append message to display"""
        ts = datetime.now().strftime("%H:%M:%S")
        roleColors = {
            "user": COLOR_USER,
            "sys": COLOR_SYS
        }
        c = roleColors.get(role, COLOR_TEXT)

        html = (
            f'<span style="color: {COLOR_TIMESTAMP};">[{ts}] </span>'
            f'<span style="color:{c};">{htmlEscape(role)} &gt; {htmlEscape(text)}</span>'
        )
        self.display.append(html)
        self._scrollToBottom()

    def appendColored(self, role: str, text: str, color: str, prefix: str = ""):
        """Append colored message with depictive prefix"""
        ts = datetime.now().strftime("%H:%M:%S")

        html = (
            f'<span style="color: {COLOR_TIMESTAMP};">[{ts}] </span>'
            f'<span style="color:{color};">{htmlEscape(prefix)} {htmlEscape(role)} &gt; {htmlEscape(text)}</span>'
        )
        self.display.append(html)
        self._scrollToBottom()

    def _scrollToBottom(self):
        """Scroll display to bottom"""
        self.display.moveCursor(QTextCursor.MoveOperation.End)
        self.display.ensureCursorVisible()

    # Sensory callbacks
    def _onKeyPress(self, key: str, timestamp: float):
        """Handle keyboard event"""
        self.sensoryContext['keyboard']['active'] = True
        self.journal.writeYAML('keyboard_event', {'class': key, 'time': timestamp})

    def _onTypingPattern(self, pattern: Dict):
        """Handle typing pattern detection"""
        self.sensoryContext['keyboard']['lastPattern'] = pattern
        self.journal.writeYAML('typing_pattern', pattern)

        # Could use for authentication
        if pattern['avgInterval'] < 0.1:
            self.append("sys", "‚å® Fast typing detected")

    def _onWindowActivity(self, data: Dict):
        """Handle window activity detection"""
        self.sensoryContext['screen']['changed'] = data['changed']
        self.sensoryContext['screen']['process'] = data['process']

        self.journal.writeYAML('window_activity', {
            'process': data['process'],
            'time': data['timestamp']
        })

    def _onAudioDetected(self, data: Dict):
        """Handle audio detection"""
        self.sensoryContext['audio']['active'] = True
        self.sensoryContext['audio']['level'] = data['level']
        self.journal.writeYAML('audio_activity', data)

    def _onSpeechDetected(self, text: str):
        """Handle speech transcription"""
        self.append("sys", f"‚ô´ Heard: {text}")
        self.journal.writeMarkdown("AUDIO", text)

    def _autoSave(self):
        """Auto-save learning state"""
        self.learning.save(LEARNING_PATH)

    # Window movement
    def mousePressEvent(self, event):
        if event.button() == Qt.MouseButton.LeftButton:
            self._dragPos = event.globalPosition().toPoint() - self.frameGeometry().topLeft()
        elif event.button() == Qt.MouseButton.RightButton:
            self._resizing = True
            self._resizeStartPos = event.globalPosition().toPoint()
            self._resizeStartGeo = self.geometry()

    def mouseMoveEvent(self, event):
        if (event.buttons() & Qt.MouseButton.LeftButton) and self._dragPos:
            self.move(event.globalPosition().toPoint() - self._dragPos)
        elif (event.buttons() & Qt.MouseButton.RightButton) and self._resizing:
            delta = event.globalPosition().toPoint() - self._resizeStartPos
            newW = max(SYSTEM_MIN_SIZE[0], self._resizeStartGeo.width() + delta.x())
            newH = max(SYSTEM_MIN_SIZE[1], self._resizeStartGeo.height() + delta.y())
            self.resize(newW, newH)

    def mouseReleaseEvent(self, event):
        if event.button() == Qt.MouseButton.LeftButton:
            self._dragPos = None
        elif event.button() == Qt.MouseButton.RightButton:
            self._resizing = False

    def keyPressEvent(self, event: QKeyEvent):
        """Handle key press"""
        if event.key() == Qt.Key.Key_Escape:
            self.close()
        super().keyPressEvent(event)

    def closeEvent(self, event):
        """Handle close"""
        # Stop sensors
        self.keyboardMonitor.stop()
        self.screenMonitor.stop()
        self.audioMonitor.stop()

        # Save learning state
        self.learning.save(LEARNING_PATH)

        # Close journal
        self.journal.closeSession("User closed window")

        super().closeEvent(event)


# ============================================================
# Utility Functions
# ============================================================

def htmlEscape(s: str) -> str:
    """HTML escape string"""
    return (s.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;"))


def createConsentRecord(accepted: bool):
    """Create consent record file"""
    Path(CONSENT_PATH).write_text(
        f"Shepherd Consent: {'ACCEPTED' if accepted else 'DECLINED'}\n"
        f"Timestamp: {datetime.now().isoformat()}\n"
    )


# ============================================================
# Main Entry Point
# ============================================================

def main():
    """Main initialization flow"""
    app = QApplication(sys.argv)

    # Show terms dialog FIRST - don't create anything yet
    termsDialog = TermsDialog()

    # These will be created only after acceptance
    config = None
    journal = None
    systemWindow = None

    def onAccepted():
        nonlocal config, journal, systemWindow

        print("‚úì Shepherd role accepted")

        # NOW create services (after consent)
        config = ConfigManager(CONFIG_PATH)
        journal = JournalManager(JOURNAL_MD_PATH, JOURNAL_YAML_PATH)

        # Save permissions to config before initialization
        config.data['permissions'] = termsDialog.permissions
        config.save()

        createConsentRecord(True)
        journal.writeMarkdown("CONSENT", "Shepherd accepted terms and responsibilities")
        journal.writeYAML('consent', {
            'accepted': True,
            'permissions': termsDialog.permissions
        })

        # Create system window
        systemWindow = SystemWindow(journal, config)

        termsDialog.close()
        systemWindow.show()

    def onDeclined():
        print("‚úó Shepherd role declined")

        # Write minimal consent record only
        createConsentRecord(False)

        termsDialog.close()
        app.quit()

    termsDialog.accepted.connect(onAccepted)
    termsDialog.declined.connect(onDeclined)
    termsDialog.show()

    sys.exit(app.exec())


if __name__ == "__main__":
    main()